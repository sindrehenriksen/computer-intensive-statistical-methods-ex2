<<echo=FALSE, cache=FALSE>>=
read_chunk("../code/problem3.R")
@
\section{}
\label{sec:ex3}
In exercise 2 we have run an MCMC-algorithm and generated $M = 70000$ samples. We will use these results to create plots to understand how good this algorithm was.
First we define the libraries we used.
%
<<libs2, eval = FALSE, fig.align='center', fig.width=6, fig.height=4, fig.cap="fig caption">>==
@
%
To create the figures we used \texttt{ggplot} from the \textit{tidyverse} package. This function needs data frames to operate. 
We are looking at the trace plots of $\vect{v}$, $\vect{u}$, $kappa_u$ and $\kappa_v$, with $\vect{v}$ being white noise of $\eta$. 
%
<<dfCreate, eval = FALSE>>==
@
%
%
\paragraph{(a)}
To start out we look at trace plots with a chosen burn in period removed. The size of the burn-in is chosen by looking at the trace plots. 
<<3a, eval = FALSE>>==
@
%
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/trace.pdf}
    \caption{}
    \label{fig:trace}
\end{figure}

Looking at the trace plots for the three different parameters for $\vect{u}$ and $\vect{v}$ in figure \ref{fig:trace}, we see that the parameters explore much of the domain, and have look like a band. This is a good indication of convergence of the MCMC. As for $\kappa_u$ and $\kappa_v$ they seem to explore less of the domain, and rarely moves far away from their mean. Also we see the mean of the $\vect{v}$s' is close to zero which is what we should expect for white noise. 
\paragraph{(b)}
Using the same samples we will now look at their autocorrelation. We will do this by using the R-function \texttt{acf}, which we create autocorrelation plots of the different samples.  
%
<<3b, eval = FALSE>>==
@
%
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/acf.pdf}
    \caption{}
    \label{fig:acf}
\end{figure}
%
In figure \ref{fig:acf} we can see that the autocorrelation for the $\vect{u}$s' and the $\vect{v}$s' is close to uncorrelated, but for $\kappa_u$ and $\kappa_v$ we can see a larger degree of autocorrelation. 
\paragraph{(c)}
Next we will test the convergence of the Markov chain using the \texttt{geweke.diag} function from the R-package \textbf{coda}. A geweke diagnostic is done by removing the burning, then taking the first $10\%$ and the last $50\%$ of the samples and then taking the difference in their respective means and dividing by the estimated standard error. If the samples then are asymptotically independent and drawn from a stationary distribution, this will result in a standard normal distribution.